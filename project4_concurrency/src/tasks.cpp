#include <iostream>
#include <thread>
#include <future>
#include <cmath>
#include <memory>
#include <vector>

using CLOCK = std::chrono::high_resolution_clock;


/* HEADER INFO:
* Course 5: Concurrency
* Lesson 2: Passing Data Between Threads
* Module 3: Threads vs. Tasks
*/


/* NOTE: SYNCHRONOUS VS ASYNCHRONOUS:
*
* A synchronous call is the one in which the function/call/procedure has to
* return before the next statement can be executed. What this means is that the
* function is "blocking" the rest of the code. In asynchronous calling,
* generally there would be multiple threads.
*/

/* TASK-BASED CONCURRENCY:
*
* Determining the optimal number of threads to use is a hard problem. It usually
* depends on the number of available cores wether it makes sense to execute code
* as a thread or in a sequential manner. The use of std::async (and thus tasks)
* take the burden of this decision away from the user and let the system decide
* whether to execute the code sequentially or as a thread. With tasks, the
* programmer decides what CAN be run in parallel in principle and the system
* then decides at runtime what WILL be run in parallel.
*
* With tasks, the system takes care of many details (e.g. join). With threads,
* the programmer is responsible for many details. As far as resources go,
* threads are usually more heavy-weight as they are generated by the operating
* system (OS). It takes time for the OS to be called and to allocate
* memory/stack/kernel data structures for the thread. Also, destroying the
* thread is expensive. Tasks on the other hand are more light-weight as they
* will be using a pool of already created threads (the "thread pool").
*/

// 1. remove std::promise as inout argument
double divideByNumber(double num, double denom)
{
  // print system id of worker thread
  std::cout << "\t(divideByNumber) Worker thread id = " <<
    std::this_thread::get_id() << std::endl;

  // simulate work
  std::this_thread::sleep_for(std::chrono::milliseconds(500)); 

  if (denom == 0)
    throw std::runtime_error("Exception from thread: Division by zero!");

  return num / denom;
}

void workerFunction(int n, bool supress_output)
{
  // Compute the square root of a floating point number n times.

  // print system id of worker thread
  if (!supress_output) {
    std::cout << "\t(workerFunction) Worker thread id = " <<
      std::this_thread::get_id() << std::endl;
  }
  
  // perform work
  for (int i = 0; i < n; ++i)
  {
    sqrt(12345.6789);
  }
}

long profiling(std::launch async_type, int nLoops, int nThreads)
{
  // start time measurement
  auto t1 = CLOCK::now();
  
  // launch the workerFunction tasks - compute square roots
  std::vector<std::future<void>> futures;
  for (int i = 0; i < nThreads; ++i)
  {
    futures.emplace_back(std::async(async_type, workerFunction, nLoops, true));
  }

  // wait for tasks to complete
  for (const std::future<void> &ftr : futures)
  {
    ftr.wait();
  }

  // stop time measurement and print execution time
  auto t2 = CLOCK::now();
  return std::chrono::duration_cast<std::chrono::microseconds>(t2 - t1).count();
}

int main()
{

  /* THREADS VS. TASKS: STD::ASYNC()
  *
  * A disadvantage of the promise-future approach is that it is very cumbersome
  * (and involves a lot of boilerplate code) to pass the promise to the thread
  * function using an rvalue reference and std::move. For the straight-forward
  * task of returning data or exceptions from a worker thread to the parent
  * thread however, there is a simpler and more convenient way using
  * std::async() instead of std::thread().
  * 
  * 1. We are removing the promise from the argument list as well as the try-
  * catch block. Also, the return type of the function is changed from void to
  * double as the result of the computation will be channeled back to the main
  * thread using a simple return. After these changes, the function has no
  * knowledge of threads, nor of futures or promises - it is a simple function
  * that takes two doubles as arguments and returns a double as a result.
  * 
  * 2. In the main thread, we need to replace the call to std::thread with
  * std::async. Note that async returns a future, which we will use later in the
  * code to retrieve the value that is returned by the function. A promise, as
  * with std::thread, is no longer needed, so the code becomes much shorter.
  * 
  * NOTE: We do not need to call join() anymore. With std::async, the thread
  * destructor will be called automatically - which reduces the risk of a
  * concurrency bug.
  */
  std::cout << "std::async():" << std::endl;

  // print system id of worker thread
  std::cout << "\tMain thread id = " << std::this_thread::get_id() << std::endl;

  // use async to start a task (system chooses synchronous or async execution)
  double num = 42.0, denom = 2.0;
  std::future<double> ftr = std::async(divideByNumber, num, denom);

  // force syncronous execution of divideByNumber
  std::future<double> ftr1 = std::async(std::launch::deferred, divideByNumber,
    num, denom);

  // force asyncronous execution of divideByNumber
  std::future<double> ftr2 = std::async(std::launch::async, divideByNumber,
    num, denom);

  // retrieve result within try-catch-block
  try
  {
    /* Retrieve the result - note that if ftr1 is used here, the main thread and
    * the worker thread will be the same - this is the result of forcing 
    * synchronous execution. For ftr and ftr2, the main thread and worker
    * thread will be different.
    * 
    * Also, note that until ftr1.get() is called, only two "Worker thread id"
    * lines will print - this is because the worker thread spawned by ftr1 is
    * forced to execute synchronously, and thus will not execute until ger() is
    * called on the future (ftr1).
    */
    double result = ftr.get();
    std::cout << "\tResult = " << result << std::endl;

  }
  catch (std::runtime_error err)
  {
    std::cout << err.what() << std::endl;
  }

  /* COMPARING STD::ASYNC() VS. STD::THREAD()
  *
  * Internally, std::async creates a promise, gets a future from it and runs a
  * template function that takes the promise, calls our function and then either
  * sets the value or the exception of that promise - depending on function
  * behavior. The code used internally by std::async is more or less identical
  * to the code we used in the previous example, except that this time it has
  * been generated by the compiler and it is hidden from us - which means that
  * the code we write appears much cleaner and leaner. Also, std::async makes it
  * possible to control the amount of concurrency by passing an optional launch
  * parameter, which enforces either synchronous or asynchronous behavior.
  */


  /* ASSESSING THE ADVATAGE OF PARALLEL EXECUTION:
  *
  * It must be noted that starting and managing threads takes a significant
  * amount of time. It is therefore not a general advantage if computations are
  * performed in parallel: It must be carefully weighed with regard to the
  * computational effort whether parallelization makes sense.
  * 
  * Below, a for-loop starts a configurable number of tasks that can either
  * be executed synchronously or asynchronously. As an experiment, we will now
  * use a number of different parameter settings to execute the program and
  * evaluate the time it takes to finish the computations. The idea is to gauge
  * the effect of the number of threads on the overall runtime:
  */
  std::cout << "\nTasks vs. Threads: Profiling" << std::endl;

  // Create vector container for storing profiling results
  std::vector<long> timing;

  // Compute 10e7 square roots using 5 threads
  std::cout << "\tnLoops = 10e7, nThreads = 5" << std::endl;
  int nLoops = 10e7, nThreads = 5;

  // force asynchronous execution 
  timing.emplace_back( profiling(std::launch::async, nLoops, nThreads) );
  // force synchronous execution 
  timing.emplace_back( profiling(std::launch::deferred, nLoops, nThreads) );

  std::cout << "\t\tstd::launch::async\t" << timing[0] << " us" << std::endl;
  std::cout << "\t\tstd::launch::deferred\t" << timing[1] << " us" << std::endl;

  /* Next, we will compute only 10 spuare roots (as opposed to 10e6) to show
  * how the overhead of starting and managing threads can be significant. Only
  * some tasks are well-suited to parallelization.
  */
  nLoops = 10, nThreads = 5;

  // Compute 10e7 square roots using 5 threads
  std::cout << "\tnLoops = 10, nThreads = 5" << std::endl;

  // force asynchronous execution 
  timing.emplace_back( profiling(std::launch::async, nLoops, nThreads) );
  // force synchronous execution 
  timing.emplace_back( profiling(std::launch::deferred, nLoops, nThreads) );

  std::cout << "\t\tstd::launch::async\t" << timing[2] << " us" << std::endl;
  std::cout << "\t\tstd::launch::deferred\t" << timing[3] << " us" << std::endl;

  return 0;
}